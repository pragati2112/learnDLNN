{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries-------\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#### Define the quadratic and cross-entropy cost functions\n",
    "\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "     \n",
    "        return (a-y)\n",
    "\n",
    "\n",
    "#### Main Network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "       \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "     \n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "    \n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "    \n",
    "        if evaluation_data:\n",
    "            evaluation_data = list(evaluation_data)\n",
    "            n_data = len(evaluation_data)\n",
    "            \n",
    "        training_data = list(training_data)    \n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        xrange = range\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print (\"Epoch %s training complete\" % j)\n",
    "            \n",
    "            \n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print( \"Cost on training data: {}\".format(cost))\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print (\"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n))\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print( \"Cost on evaluation data: {}\".format(cost))\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print (\"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data))\n",
    "\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        xrange = range\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        \n",
    "        xrange = range\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "       \n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "\n",
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "    and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "    into a corresponding desired output from the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "def load_data():\n",
    "    f = gzip.open('./data/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 0.5007273811684175\n",
      "Accuracy on training data: 46758 / 50000\n",
      "Cost on evaluation data: 0.8185118444184842\n",
      "Accuracy on evaluation data: 9341 / 10000\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 0.4345115718555912\n",
      "Accuracy on training data: 47569 / 50000\n",
      "Cost on evaluation data: 0.8486328458411189\n",
      "Accuracy on evaluation data: 9473 / 10000\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 0.40844455614516884\n",
      "Accuracy on training data: 47837 / 50000\n",
      "Cost on evaluation data: 0.8697993777932762\n",
      "Accuracy on evaluation data: 9513 / 10000\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 0.4093040802606078\n",
      "Accuracy on training data: 47976 / 50000\n",
      "Cost on evaluation data: 0.9024712542373676\n",
      "Accuracy on evaluation data: 9541 / 10000\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.4288557808545558\n",
      "Accuracy on training data: 47795 / 50000\n",
      "Cost on evaluation data: 0.9505948481993508\n",
      "Accuracy on evaluation data: 9481 / 10000\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.41198455928221633\n",
      "Accuracy on training data: 48009 / 50000\n",
      "Cost on evaluation data: 0.9366116439373333\n",
      "Accuracy on evaluation data: 9537 / 10000\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.3965131655360419\n",
      "Accuracy on training data: 48146 / 50000\n",
      "Cost on evaluation data: 0.9264602635193568\n",
      "Accuracy on evaluation data: 9549 / 10000\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.40480313675264346\n",
      "Accuracy on training data: 48090 / 50000\n",
      "Cost on evaluation data: 0.950639048171747\n",
      "Accuracy on evaluation data: 9536 / 10000\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.37249328146183247\n",
      "Accuracy on training data: 48338 / 50000\n",
      "Cost on evaluation data: 0.9303266767254745\n",
      "Accuracy on evaluation data: 9586 / 10000\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.374973104284816\n",
      "Accuracy on training data: 48377 / 50000\n",
      "Cost on evaluation data: 0.9361231327898469\n",
      "Accuracy on evaluation data: 9584 / 10000\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 0.3744905445747292\n",
      "Accuracy on training data: 48332 / 50000\n",
      "Cost on evaluation data: 0.934077973446794\n",
      "Accuracy on evaluation data: 9577 / 10000\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 0.37205298642917906\n",
      "Accuracy on training data: 48413 / 50000\n",
      "Cost on evaluation data: 0.9340239787137936\n",
      "Accuracy on evaluation data: 9609 / 10000\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 0.40674331512220807\n",
      "Accuracy on training data: 48137 / 50000\n",
      "Cost on evaluation data: 0.9711267588629129\n",
      "Accuracy on evaluation data: 9532 / 10000\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 0.37931804324512297\n",
      "Accuracy on training data: 48379 / 50000\n",
      "Cost on evaluation data: 0.9395213123192919\n",
      "Accuracy on evaluation data: 9618 / 10000\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 0.39249600618942976\n",
      "Accuracy on training data: 48232 / 50000\n",
      "Cost on evaluation data: 0.9649526086398126\n",
      "Accuracy on evaluation data: 9554 / 10000\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.39582549194371275\n",
      "Accuracy on training data: 48258 / 50000\n",
      "Cost on evaluation data: 0.9607145588875023\n",
      "Accuracy on evaluation data: 9583 / 10000\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.3686212479953462\n",
      "Accuracy on training data: 48443 / 50000\n",
      "Cost on evaluation data: 0.9461302839469548\n",
      "Accuracy on evaluation data: 9608 / 10000\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.38081575947073815\n",
      "Accuracy on training data: 48323 / 50000\n",
      "Cost on evaluation data: 0.9552988959895788\n",
      "Accuracy on evaluation data: 9592 / 10000\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.37848071862630583\n",
      "Accuracy on training data: 48417 / 50000\n",
      "Cost on evaluation data: 0.9462314188118546\n",
      "Accuracy on evaluation data: 9585 / 10000\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.37692565104538767\n",
      "Accuracy on training data: 48351 / 50000\n",
      "Cost on evaluation data: 0.9545060208715314\n",
      "Accuracy on evaluation data: 9570 / 10000\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.37016974964542454\n",
      "Accuracy on training data: 48405 / 50000\n",
      "Cost on evaluation data: 0.9485802279322217\n",
      "Accuracy on evaluation data: 9606 / 10000\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.37948221745746347\n",
      "Accuracy on training data: 48298 / 50000\n",
      "Cost on evaluation data: 0.9649117987233873\n",
      "Accuracy on evaluation data: 9571 / 10000\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.3741708546214632\n",
      "Accuracy on training data: 48370 / 50000\n",
      "Cost on evaluation data: 0.9546469310816695\n",
      "Accuracy on evaluation data: 9581 / 10000\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.35657848146397597\n",
      "Accuracy on training data: 48556 / 50000\n",
      "Cost on evaluation data: 0.9414435761381268\n",
      "Accuracy on evaluation data: 9611 / 10000\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.3766062103110883\n",
      "Accuracy on training data: 48356 / 50000\n",
      "Cost on evaluation data: 0.9609581319732994\n",
      "Accuracy on evaluation data: 9584 / 10000\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.38198337921833414\n",
      "Accuracy on training data: 48358 / 50000\n",
      "Cost on evaluation data: 0.9630646877496214\n",
      "Accuracy on evaluation data: 9587 / 10000\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.37859433050063074\n",
      "Accuracy on training data: 48415 / 50000\n",
      "Cost on evaluation data: 0.9520227354599167\n",
      "Accuracy on evaluation data: 9590 / 10000\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.38117651633622474\n",
      "Accuracy on training data: 48369 / 50000\n",
      "Cost on evaluation data: 0.9551476172008463\n",
      "Accuracy on evaluation data: 9605 / 10000\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.35832829007788786\n",
      "Accuracy on training data: 48508 / 50000\n",
      "Cost on evaluation data: 0.9343999608144954\n",
      "Accuracy on evaluation data: 9624 / 10000\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.36623766620977666\n",
      "Accuracy on training data: 48473 / 50000\n",
      "Cost on evaluation data: 0.9465126611321546\n",
      "Accuracy on evaluation data: 9593 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.8185118444184842,\n",
       "  0.8486328458411189,\n",
       "  0.8697993777932762,\n",
       "  0.9024712542373676,\n",
       "  0.9505948481993508,\n",
       "  0.9366116439373333,\n",
       "  0.9264602635193568,\n",
       "  0.950639048171747,\n",
       "  0.9303266767254745,\n",
       "  0.9361231327898469,\n",
       "  0.934077973446794,\n",
       "  0.9340239787137936,\n",
       "  0.9711267588629129,\n",
       "  0.9395213123192919,\n",
       "  0.9649526086398126,\n",
       "  0.9607145588875023,\n",
       "  0.9461302839469548,\n",
       "  0.9552988959895788,\n",
       "  0.9462314188118546,\n",
       "  0.9545060208715314,\n",
       "  0.9485802279322217,\n",
       "  0.9649117987233873,\n",
       "  0.9546469310816695,\n",
       "  0.9414435761381268,\n",
       "  0.9609581319732994,\n",
       "  0.9630646877496214,\n",
       "  0.9520227354599167,\n",
       "  0.9551476172008463,\n",
       "  0.9343999608144954,\n",
       "  0.9465126611321546],\n",
       " [9341,\n",
       "  9473,\n",
       "  9513,\n",
       "  9541,\n",
       "  9481,\n",
       "  9537,\n",
       "  9549,\n",
       "  9536,\n",
       "  9586,\n",
       "  9584,\n",
       "  9577,\n",
       "  9609,\n",
       "  9532,\n",
       "  9618,\n",
       "  9554,\n",
       "  9583,\n",
       "  9608,\n",
       "  9592,\n",
       "  9585,\n",
       "  9570,\n",
       "  9606,\n",
       "  9571,\n",
       "  9581,\n",
       "  9611,\n",
       "  9584,\n",
       "  9587,\n",
       "  9590,\n",
       "  9605,\n",
       "  9624,\n",
       "  9593],\n",
       " [0.5007273811684175,\n",
       "  0.4345115718555912,\n",
       "  0.40844455614516884,\n",
       "  0.4093040802606078,\n",
       "  0.4288557808545558,\n",
       "  0.41198455928221633,\n",
       "  0.3965131655360419,\n",
       "  0.40480313675264346,\n",
       "  0.37249328146183247,\n",
       "  0.374973104284816,\n",
       "  0.3744905445747292,\n",
       "  0.37205298642917906,\n",
       "  0.40674331512220807,\n",
       "  0.37931804324512297,\n",
       "  0.39249600618942976,\n",
       "  0.39582549194371275,\n",
       "  0.3686212479953462,\n",
       "  0.38081575947073815,\n",
       "  0.37848071862630583,\n",
       "  0.37692565104538767,\n",
       "  0.37016974964542454,\n",
       "  0.37948221745746347,\n",
       "  0.3741708546214632,\n",
       "  0.35657848146397597,\n",
       "  0.3766062103110883,\n",
       "  0.38198337921833414,\n",
       "  0.37859433050063074,\n",
       "  0.38117651633622474,\n",
       "  0.35832829007788786,\n",
       "  0.36623766620977666],\n",
       " [46758,\n",
       "  47569,\n",
       "  47837,\n",
       "  47976,\n",
       "  47795,\n",
       "  48009,\n",
       "  48146,\n",
       "  48090,\n",
       "  48338,\n",
       "  48377,\n",
       "  48332,\n",
       "  48413,\n",
       "  48137,\n",
       "  48379,\n",
       "  48232,\n",
       "  48258,\n",
       "  48443,\n",
       "  48323,\n",
       "  48417,\n",
       "  48351,\n",
       "  48405,\n",
       "  48298,\n",
       "  48370,\n",
       "  48556,\n",
       "  48356,\n",
       "  48358,\n",
       "  48415,\n",
       "  48369,\n",
       "  48508,\n",
       "  48473])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data , validation_data , test_data = load_data_wrapper()\n",
    "net = Network([784, 30, 10], cost=CrossEntropyCost)\n",
    "net.SGD(training_data, 30, 10, 0.5,lmbda = 5.0, evaluation_data=test_data, monitor_evaluation_accuracy=True, monitor_evaluation_cost=True, monitor_training_accuracy=True, monitor_training_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}